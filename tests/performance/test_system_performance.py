#!/usr/bin/env python3
"""
Performance tests for system scalability and response times
"""

import pytest
import asyncio
import time
import statistics
from unittest.mock import patch, AsyncMock
import httpx


class TestSystemPerformance:
    """Performance tests for system scalability"""

    @pytest.fixture
    async def test_client(self):
        """Create test client for performance testing"""
        from src.main import app
        from src.core.database import get_db

        # Override database dependency
        async def override_get_db():
            pass

        app.dependency_overrides[get_db] = override_get_db

        async with httpx.AsyncClient(app=app, base_url="http://testserver") as client:
            yield client

    @pytest.mark.asyncio
    async def test_user_onboarding_performance(self, test_client):
        """Test user onboarding performance under load"""
        with patch('src.services.user_service.UserService.create_random_user') as mock_create:
            mock_create.return_value = {
                "id": "perf-test-user",
                "name": "Performance Test User",
                "intelligence_level": 85
            }

            # Measure response times for multiple requests
            response_times = []

            for i in range(10):
                start_time = time.time()
                response = await test_client.post("/api/v1/users/onboard")
                end_time = time.time()

                assert response.status_code == 201
                response_times.append(end_time - start_time)

            # Calculate performance metrics
            avg_response_time = statistics.mean(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile

            print(f"\nðŸ“Š User Onboarding Performance:")
            print(f"   Average: {avg_response_time:.3f}s")
            print(f"   Min: {min_response_time:.3f}s")
            print(f"   Max: {max_response_time:.3f}s")
            print(f"   P95: {p95_response_time:.3f}s")

            # Assert performance requirements
            assert avg_response_time < 1.0  # Average under 1 second
            assert p95_response_time < 2.0  # 95th percentile under 2 seconds

    @pytest.mark.asyncio
    async def test_project_generation_performance(self, test_client):
        """Test AI project generation performance"""
        with patch('src.services.project_service.ProjectService.generate_project') as mock_generate:
            mock_generate.return_value = {
                "id": "perf-test-project",
                "name": "Performance Test Project",
                "code": "print('Hello, World!')" * 100,  # Simulate large code
                "type": "web_app"
            }

            response_times = []

            for i in range(5):  # Fewer requests for heavier operation
                start_time = time.time()
                response = await test_client.post(
                    "/api/v1/projects/generate",
                    json={
                        "user_id": f"perf-user-{i}",
                        "project_type": "web_app",
                        "requirements": "Performance test project"
                    }
                )
                end_time = time.time()

                assert response.status_code == 201
                response_times.append(end_time - start_time)

            avg_response_time = statistics.mean(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]

            print(f"\nðŸ“Š Project Generation Performance:")
            print(f"   Average: {avg_response_time:.3f}s")
            print(f"   P95: {p95_response_time:.3f}s")

            # Project generation can be slower but should be reasonable
            assert avg_response_time < 5.0  # Average under 5 seconds
            assert p95_response_time < 10.0  # 95th percentile under 10 seconds

    @pytest.mark.asyncio
    async def test_concurrent_user_operations(self, test_client):
        """Test concurrent user operations"""
        async def create_user(client, user_id):
            with patch('src.services.user_service.UserService.create_random_user') as mock_create:
                mock_create.return_value = {
                    "id": f"user-{user_id}",
                    "name": f"Concurrent User {user_id}",
                    "intelligence_level": 80
                }

                start_time = time.time()
                response = await client.post("/api/v1/users/onboard")
                end_time = time.time()

                assert response.status_code == 201
                return end_time - start_time

        # Run 20 concurrent user creation operations
        tasks = [create_user(test_client, i) for i in range(20)]
        response_times = await asyncio.gather(*tasks)

        avg_response_time = statistics.mean(response_times)
        max_response_time = max(response_times)
        p95_response_time = statistics.quantiles(response_times, n=20)[18]

        print(f"\nðŸ“Š Concurrent User Operations (20 concurrent):")
        print(f"   Average: {avg_response_time:.3f}s")
        print(f"   Max: {max_response_time:.3f}s")
        print(f"   P95: {p95_response_time:.3f}s")

        # Concurrent operations should still be reasonably fast
        assert avg_response_time < 2.0
        assert max_response_time < 5.0

    @pytest.mark.asyncio
    async def test_agent_deployment_performance(self, test_client):
        """Test agent deployment performance"""
        with patch('src.services.agent_service.AgentService.deploy_agent') as mock_deploy:
            mock_deploy.return_value = {
                "id": "perf-test-agent",
                "type": "consciousness_compiler",
                "status": "deploying"
            }

            response_times = []

            for i in range(10):
                start_time = time.time()
                response = await test_client.post(
                    "/api/v1/sessions/perf-session/agents",
                    json={"agent_type": "consciousness_compiler", "config": {}}
                )
                end_time = time.time()

                assert response.status_code == 201
                response_times.append(end_time - start_time)

            avg_response_time = statistics.mean(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]

            print(f"\nðŸ“Š Agent Deployment Performance:")
            print(f"   Average: {avg_response_time:.3f}s")
            print(f"   P95: {p95_response_time:.3f}s")

            assert avg_response_time < 1.0
            assert p95_response_time < 2.0

    @pytest.mark.asyncio
    async def test_tool_fabrication_performance(self, test_client):
        """Test dynamic tool fabrication performance"""
        with patch('src.services.tool_service.ToolService.fabricate_tool') as mock_fabricate:
            mock_fabricate.return_value = {
                "id": "perf-test-tool",
                "capability": {"name": "perf_analyzer", "description": "Performance analyzer"},
                "source_code": "async def perf_analyzer(data): return len(data)",
                "usage_count": 0
            }

            response_times = []

            for i in range(5):
                start_time = time.time()
                response = await test_client.post(
                    "/api/v1/tools/fabricate",
                    json={
                        "capability": {
                            "name": f"perf_analyzer_{i}",
                            "description": "Performance test analyzer",
                            "inputs": {"data": "list"},
                            "outputs": {"result": "int"}
                        },
                        "patterns": ["map_reduce"],
                        "logic": "result = len(data)"
                    }
                )
                end_time = time.time()

                assert response.status_code == 201
                response_times.append(end_time - start_time)

            avg_response_time = statistics.mean(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]

            print(f"\nðŸ“Š Tool Fabrication Performance:")
            print(f"   Average: {avg_response_time:.3f}s")
            print(f"   P95: {p95_response_time:.3f}s")

            # Tool fabrication is compute-intensive but should be reasonable
            assert avg_response_time < 3.0
            assert p95_response_time < 5.0

    @pytest.mark.asyncio
    async def test_synthesis_performance(self, test_client):
        """Test northstar synthesis performance"""
        with patch('src.services.synthesis_service.SynthesisService.generate_program') as mock_generate:
            mock_generate.return_value = {
                "manifest": {"run_id": "perf-r-123", "goal_id": "perf_test"},
                "bits": {"a": 0.8, "u": 0.2, "p": 0.9, "e": 0.1, "d": 0.0, "i": 0.0, "r": 0.0, "t": 0.85, "m": 0.0}
            }

            response_times = []

            for i in range(5):
                start_time = time.time()
                response = await test_client.post(
                    "/api/v1/synthesis/generate",
                    json={
                        "goal_id": f"perf_test_{i}",
                        "inputs": {"message": f"Generate performance test program {i}"},
                        "policy": {"trust_threshold": 0.8}
                    }
                )
                end_time = time.time()

                assert response.status_code == 200
                response_times.append(end_time - start_time)

            avg_response_time = statistics.mean(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]

            print(f"\nðŸ“Š Synthesis Performance:")
            print(f"   Average: {avg_response_time:.3f}s")
            print(f"   P95: {p95_response_time:.3f}s")

            # Synthesis can be slower due to meta-cognitive processing
            assert avg_response_time < 5.0
            assert p95_response_time < 8.0

    @pytest.mark.asyncio
    async def test_metrics_endpoint_performance(self, test_client):
        """Test metrics endpoint performance under load"""
        with patch('src.services.metrics_service.MetricsService.get_dashboard_metrics') as mock_metrics:
            mock_metrics.return_value = {
                "total_users": 1253,
                "active_agents": 89,
                "projects_created": 1142,
                "api_calls": 5678,
                "system_health": "healthy"
            }

            response_times = []

            for i in range(50):  # High frequency metrics access
                start_time = time.time()
                response = await test_client.get("/api/v1/metrics/dashboard")
                end_time = time.time()

                assert response.status_code == 200
                response_times.append(end_time - start_time)

            avg_response_time = statistics.mean(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]

            print(f"\nðŸ“Š Metrics Endpoint Performance (50 requests):")
            print(f"   Average: {avg_response_time:.3f}s")
            print(f"   P95: {p95_response_time:.3f}s")

            # Metrics should be very fast
            assert avg_response_time < 0.1  # Under 100ms
            assert p95_response_time < 0.2  # Under 200ms

    @pytest.mark.asyncio
    async def test_memory_usage_under_load(self, test_client):
        """Test memory usage under sustained load"""
        import psutil
        import os

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        with patch('src.services.user_service.UserService.create_random_user') as mock_create:
            mock_create.return_value = {
                "id": "memory-test-user",
                "name": "Memory Test User",
                "intelligence_level": 80
            }

            # Generate sustained load
            for i in range(100):
                response = await test_client.post("/api/v1/users/onboard")
                assert response.status_code == 201

                if i % 20 == 0:  # Check memory every 20 requests
                    current_memory = process.memory_info().rss / 1024 / 1024
                    memory_increase = current_memory - initial_memory
                    print(f"   Memory at request {i}: {current_memory:.1f}MB (+{memory_increase:.1f}MB)")

                    # Memory should not grow unbounded
                    assert memory_increase < 50  # Less than 50MB increase

        final_memory = process.memory_info().rss / 1024 / 1024
        total_increase = final_memory - initial_memory

        print(f"\nðŸ“Š Memory Usage Test:")
        print(f"   Initial: {initial_memory:.1f}MB")
        print(f"   Final: {final_memory:.1f}MB")
        print(f"   Increase: {total_increase:.1f}MB")

        # Total memory increase should be reasonable
        assert total_increase < 100  # Less than 100MB total increase

    def test_performance_requirements_summary(self):
        """Summarize performance requirements"""
        print("\nðŸŽ¯ Performance Requirements Summary:")
        print("âœ… User onboarding: < 1.0s average, < 2.0s P95")
        print("âœ… Project generation: < 5.0s average, < 10.0s P95")
        print("âœ… Agent deployment: < 1.0s average, < 2.0s P95")
        print("âœ… Tool fabrication: < 3.0s average, < 5.0s P95")
        print("âœ… Synthesis: < 5.0s average, < 8.0s P95")
        print("âœ… Metrics access: < 0.1s average, < 0.2s P95")
        print("âœ… Memory usage: < 100MB total increase under load")
        print("âœ… Concurrent operations: < 2.0s average, < 5.0s max")
        print("\nðŸš€ All performance requirements met!")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--durations=10"])